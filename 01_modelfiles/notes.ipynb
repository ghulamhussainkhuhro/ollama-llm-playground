{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7acd5e7f-e180-4d70-9bd0-b1e7863791e8",
   "metadata": {},
   "source": [
    "# üìù Ollama Notes - Custom Models & Commands\n",
    "A quick guide to running, customizing, and debugging Ollama models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788faec1-c0d9-46af-98d6-764618fe824a",
   "metadata": {},
   "source": [
    "# üöÄ Introduction to Ollama\n",
    "Ollama is a tool that makes it easy to **run Large Language Models (LLMs) locally** on your own machine.  \n",
    "Instead of always relying on cloud APIs (like OpenAI, Anthropic, etc.), Ollama lets you:\n",
    "- Run models like Llama 3, Mistral, Gemma, etc.\n",
    "- Customize them with your own instructions.\n",
    "- Use them offline without sending data to external servers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887e249-0573-4fba-92e3-5ea1a434943c",
   "metadata": {},
   "source": [
    "## üåç Why Ollama Matters\n",
    "- **Privacy** ‚Üí Data stays on your device, no leaks.\n",
    "- **Cost-saving** ‚Üí No API bills, run models for free once downloaded.\n",
    "- **Customization** ‚Üí Easily build your own model personalities using Modelfiles.\n",
    "- **Experimentation** ‚Üí Test multiple open-source LLMs locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2457ba6f-f31b-4a97-bcd9-90ac0608ecd3",
   "metadata": {},
   "source": [
    "## üè≠ Industry Use Cases of Ollama\n",
    "1. **Prototyping AI Products** ‚Üí Quickly test prompts and personalities before scaling.\n",
    "2. **Local Agents** ‚Üí Build personal assistants without API limits.\n",
    "3. **Enterprise Security** ‚Üí Companies keep sensitive data in-house.\n",
    "4. **Education** ‚Üí Students/Researchers explore LLMs without cloud dependency.\n",
    "5. **AI Communities** ‚Üí Share customized models (e.g., roasting bots, tutors).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61046c74-4690-49ce-b682-03ae7c068a24",
   "metadata": {},
   "source": [
    "## üîë Ollama vs Cloud APIs\n",
    "- **Ollama** ‚Üí Runs locally, free after download, limited by your hardware.\n",
    "- **APIs (OpenAI, Groq, Anthropic)** ‚Üí Cloud power, larger models, but costs money.\n",
    "üëâ Think of Ollama as a **personal playground** for LLMs before going enterprise-scale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9785868-803c-4a1c-8dfe-251e8990459f",
   "metadata": {},
   "source": [
    "## üß± Core Concepts\n",
    "- **Model** ‚Üí Pretrained LLM (e.g., Llama 3.2).\n",
    "- **Modelfile** ‚Üí A config file to customize how the model behaves.\n",
    "- **Parameters** ‚Üí Settings like temperature, top_p, top_k that control creativity.\n",
    "- **System Prompt** ‚Üí Defines the bot‚Äôs personality and tone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c827c0c6-65dc-4a40-9d82-33584d662270",
   "metadata": {},
   "source": [
    "## ‚ö° Advantages of Ollama\n",
    "- Very **easy to install and run** (`ollama run llama3.2`).\n",
    "- Built-in **prompting and fine-tuning** options.\n",
    "- Models are **optimized for laptops** (uses GPU if available).\n",
    "- Can connect with frameworks like **LangChain, LlamaIndex, and FastAPI**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5492c08e-5299-443e-bc88-263d403555ac",
   "metadata": {},
   "source": [
    "## üîß Practical Use Cases You Can Build\n",
    "- **Chatbots** ‚Üí Customer support, personal assistants.\n",
    "- **Creative Writing** ‚Üí Story generation, dark humor bots, content ideas.\n",
    "- **Coding Help** ‚Üí Local AI pair programmer.\n",
    "- **Education Tools** ‚Üí Tutors that never stop roasting üòà.\n",
    "- **Voice Agents** ‚Üí Combine with STT/TTS for Jarvis-like bots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e33860-1e2e-4827-b24a-16739da20e25",
   "metadata": {},
   "source": [
    "## üè¢ How Industry Actually Uses Ollama\n",
    "- **Startups** ‚Üí Rapid prototyping of AI tools before scaling with APIs.\n",
    "- **Researchers** ‚Üí Experiment with open-source models for benchmarking.\n",
    "- **Enterprises** ‚Üí Internal-only assistants (legal, medical, banking).\n",
    "- **Creators** ‚Üí Make niche bots (fitness coach, therapist, meme generator).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c029e2ce-e8e5-4d48-a49a-8e175d79432f",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è Limitations to Keep in Mind\n",
    "- Dependent on your **local hardware (RAM + GPU)**.\n",
    "- Models are smaller than enterprise cloud models ‚Üí may feel less powerful.\n",
    "- Limited ecosystem compared to cloud LLM APIs.\n",
    "- Not always production-ready ‚Üí mainly for testing and personal use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d96caf-274e-4648-bde7-4fb151e27759",
   "metadata": {},
   "source": [
    "## üéØ What You Should Focus On (as a learner)\n",
    "1. Master **basic Ollama commands**.\n",
    "2. Learn to **create and customize models** with Modelfiles.\n",
    "3. Experiment with **different parameters** (temperature, top_p, etc.).\n",
    "4. Build **fun use cases** (roasting bots, concise tutors).\n",
    "5. Later ‚Üí Connect with **LangChain or FastAPI** for real-world apps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebe61d2-c980-4c6b-aec8-6f74d36a2c17",
   "metadata": {},
   "source": [
    "## üîπ Basic Ollama Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48c8c2c-d195-4977-b444-5557de53fedf",
   "metadata": {},
   "source": [
    "### Check installed models\n",
    "ollama list\n",
    "\n",
    "### Run a default model\n",
    "ollama run llama3.2\n",
    "\n",
    "### Pull a model from Ollama registry\n",
    "ollama pull llama3.2\n",
    "\n",
    "### Delete a model\n",
    "ollama rm llama3.2\n",
    "\n",
    "### Show logs (useful for debugging)\n",
    "ollama logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5869660-4888-43c6-a31f-6ec64d1ef25f",
   "metadata": {},
   "source": [
    "# üîπ Creating a Custom Model (Modelfile)\n",
    "1. **Create a Modelfile**  \n",
    "   Example: `roast.Modelfile`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293089e7-dd83-479f-8b07-e0caabae4a5e",
   "metadata": {},
   "source": [
    "FROM llama3.2\n",
    "\n",
    "PARAMETER temperature 0.7\n",
    "\n",
    "SYSTEM \"\"\"\n",
    "You are a sarcastic roast bot.\n",
    "Roast the user brutally before answering their question.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e0779-4146-4269-a0bc-bc81329fbfc3",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "ollama create roastbot -f roast.Modelfile\n",
    "\n",
    "# Run it\n",
    "ollama run roastbot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562ffea0-6923-43d9-85a7-a2eacfc8ce0e",
   "metadata": {},
   "source": [
    "## üîπ Common Parameters Explained\n",
    "\n",
    "- **`temperature`** ‚Üí randomness.  \n",
    "  - Low (0.2‚Äì0.4) = focused, boring  \n",
    "  - Medium (0.6) = spicy, balanced  \n",
    "  - High (0.9‚Äì1.2) = chaotic, meme-tier  \n",
    "\n",
    "- **`top_p`** ‚Üí restricts token choices by probability (nucleus sampling). Lower = safer, higher = riskier.  \n",
    "\n",
    "- **`top_k`** ‚Üí only choose from the top-k most likely tokens. Smaller = more predictable.  \n",
    "\n",
    "- **`num_ctx`** ‚Üí how much memory (tokens) the model keeps in mind. Bigger = remembers more.  \n",
    "\n",
    "- **`num_predict`** ‚Üí max tokens to generate in one reply.  \n",
    "\n",
    "- **`repeat_penalty`** ‚Üí prevents the model from spamming the same roast again & again.  \n",
    "\n",
    "- **`seed`** ‚Üí same seed = same roast (useful for reproducibility).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4a194f-f353-4012-9fa5-4e442739be6e",
   "metadata": {},
   "source": [
    "## üîπ Debugging Common Errors\n",
    "\n",
    "- **`invalid float value`** ‚Üí Don‚Äôt put comments after numbers.  \n",
    "  ‚ùå `temperature 0.6 # comment`  \n",
    "  ‚úÖ `temperature 0.6`\n",
    "\n",
    "- **`unexpected EOF`** ‚Üí Always close `SYSTEM \"\"\" ... \"\"\"` properly with triple quotes and end the file with a newline.  \n",
    "\n",
    "- **Model not found** ‚Üí Run `ollama pull llama3.2` first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eac579-b3a0-4b12-a8ea-1e8499d30b1e",
   "metadata": {},
   "source": [
    "## üîπ Best Workflow\n",
    "\n",
    "1. Start simple:\n",
    "```bash\n",
    "ollama run llama3.2\n",
    "```\n",
    "2. Clone & tweak:\n",
    "\n",
    "- Make a Modelfile\n",
    "- Add personality/system instructions\n",
    "- Adjust parameters\n",
    "\n",
    "3. Build & test:\n",
    "```bash\n",
    "ollama create customname -f file.Modelfile\n",
    "ollama run customname\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce137740-c9c1-44f8-84ed-75dd2b38a384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
